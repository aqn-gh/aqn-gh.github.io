<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Aidan Nguyen" />
    
    <link rel="shortcut icon" type="image/x-icon" href="../../img/favicon.ico">
    <title>Project 1: Exploratory Data Analysis</title>
    <meta name="generator" content="Hugo 0.83.1" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="../../css/main.css" />
    <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,400,200bold,400old" />
    
    <!--[if lt IE 9]>
			<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
			<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
		<![endif]-->

    
  </head>

  <body>
    <div id="wrap">
      
      <nav class="navbar navbar-default">
  <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand" href="../../"><i class="fa fa-home"></i></a>
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <div class="navbar-collapse collapse" id="navbar">
      <ul class="nav navbar-nav navbar-right">
      
        
        <li><a href="../../post/">BLOG</a></li>
        
        <li><a href="../../projects/">PROJECTS</a></li>
        
        <li><a href="../../resume/">RESUME</a></li>
        
      
      </ul>
    </div>
  </div>
</nav>

      <div class="container">
        <div class="blog-post">
          <h3>
            <strong><a href="../../project/project1/">Project 1: Exploratory Data Analysis</a></strong>
          </h3>
        </div>
 
<div class="blog-title">
          <h4>
         January 1, 0001 
            &nbsp;&nbsp;
            
          </h4>
        </div>

        <div class="panel panel-default">
          <div class="panel-body">
            <div class="blogpost">
              


<pre class="r"><code># install.packages(&#39;fivethirtyeight&#39;)
library(fivethirtyeight)
library(tidyverse)
library(ggplot2)
library(cluster)
data(hate_crimes)
data(police_killings)</code></pre>
<div id="introduction" class="section level2">
<h2>0. Introduction</h2>
<p>As a first generation Asian-American, it is heartbreaking to see the increasing amount of division and hate spread throughout The United States of America. This project will analyze two data sets. The first data set observes hate crimes committed in each state and data representative of their respective state such as:the median household income in 2016, the share of the population that is unemployed in 2016, the share of the population that lives in metropolitan areas in 2015, the share of adults that are 25 and older with a high-school degree in 2009, the share of the population that are U.S. citizens in 2015, the share of white residents who are living in poverty in 2015, the Gini Index in 2015, the share of the population that is not white in 2015, the share of 2016 U.S. presidential voters who voted for Donald Trump, Hate crimes per 100,000 population through November 9-18 in 2016, and the average annual hate crimes per 100,000 population between 2010 and 2015. This data was acquired through publicly available data sources via FBI hate crimes data and the Southern Poverty Law Center. The second data set contains information about victims of police shootings such as their name, their gender, their race and ethnicity, when they were murdered, and where they were murdered. This data was acquired from an interactive database titled “The Counted” which was published by the Guardian. I chose these data sets to construct my project over because as previously stated, I am an Asian-American. The U.S. is seeing an astronomical uptick in hate crimes committed against not only immigrants, but all People of Color. I have decided to base my project over these two data sets in hopes of bettering my understanding of the relationship between occurrences of hate crimes and occurrences of police brutality. A potential association I am expecting to observe is states with high levels of hate crimes committed also seeing high levels of fatal police shootings.</p>
</div>
<div id="tidying" class="section level2">
<h2>1. Tidying</h2>
<p>Data is already tidy, tidying used in correlation heatmap.</p>
</div>
<div id="joiningmerging" class="section level2">
<h2>2. Joining/Merging</h2>
<pre class="r"><code># Creation of merged data set
Merged_Data &lt;- hate_crimes %&gt;% full_join(police_killings, by = c(state_abbrev = &quot;state&quot;)) %&gt;% 
    na.omit
Merged_Data %&gt;% na.omit</code></pre>
<pre><code>## # A tibble: 400 x 46
##    state state_abbrev median_house_inc share_unemp_seas share_pop_metro
##    &lt;chr&gt; &lt;chr&gt;                   &lt;int&gt;            &lt;dbl&gt;           &lt;dbl&gt;
##  1 Alab… AL                      42278            0.06             0.64
##  2 Alab… AL                      42278            0.06             0.64
##  3 Alab… AL                      42278            0.06             0.64
##  4 Alab… AL                      42278            0.06             0.64
##  5 Alab… AL                      42278            0.06             0.64
##  6 Alab… AL                      42278            0.06             0.64
##  7 Alab… AL                      42278            0.06             0.64
##  8 Alab… AL                      42278            0.06             0.64
##  9 Alas… AK                      67629            0.064            0.63
## 10 Alas… AK                      67629            0.064            0.63
## # … with 390 more rows, and 41 more variables: share_pop_hs &lt;dbl&gt;,
## #   share_non_citizen &lt;dbl&gt;, share_white_poverty &lt;dbl&gt;, gini_index &lt;dbl&gt;,
## #   share_non_white &lt;dbl&gt;, share_vote_trump &lt;dbl&gt;,
## #   hate_crimes_per_100k_splc &lt;dbl&gt;, avg_hatecrimes_per_100k_fbi &lt;dbl&gt;,
## #   name &lt;chr&gt;, age &lt;int&gt;, gender &lt;chr&gt;, raceethnicity &lt;chr&gt;, month &lt;chr&gt;,
## #   day &lt;int&gt;, year &lt;int&gt;, streetaddress &lt;chr&gt;, city &lt;chr&gt;, latitude &lt;dbl&gt;,
## #   longitude &lt;dbl&gt;, state_fp &lt;int&gt;, county_fp &lt;int&gt;, tract_ce &lt;int&gt;,
## #   geo_id &lt;dbl&gt;, county_id &lt;int&gt;, namelsad &lt;chr&gt;, lawenforcementagency &lt;chr&gt;,
## #   cause &lt;chr&gt;, armed &lt;chr&gt;, pop &lt;int&gt;, share_white &lt;dbl&gt;, share_black &lt;dbl&gt;,
## #   share_hispanic &lt;dbl&gt;, p_income &lt;int&gt;, h_income &lt;int&gt;, county_income &lt;int&gt;,
## #   comp_income &lt;dbl&gt;, county_bucket &lt;int&gt;, nat_bucket &lt;int&gt;, pov &lt;dbl&gt;,
## #   urate &lt;dbl&gt;, college &lt;dbl&gt;</code></pre>
<p>I used full_join to completely merge the two data sets based on their common variable of the state in which either the hate crime or the fatal police shooting occurred. The hate_crimes data set contained 51 observations, 1 for each state with an additional observation for the District of Columbia. The police_killings data set contained 467 observations, 1 for each victim of police brutality. I chose the state variable because I want to isolate the location in which these two afflictions occur. With the two data sets merged, it will be easier to highlight states that have high levels of deaths due to police. If a particular state has a relatively high level of hate crimes committed in it, I want to know if fatal police activity also occur at high rates and whether other states would follow this trend. No cases were dropped.</p>
</div>
<div id="wrangling" class="section level2">
<h2>3. Wrangling</h2>
<pre class="r"><code># Usage of the 6 core dplyer functions (filter, select,
# arrange, group_by, mutate, summarize)
Merged_Data %&gt;% filter(state_abbrev == &quot;TX&quot;) %&gt;% summarize(victims = n())</code></pre>
<pre><code>## # A tibble: 1 x 1
##   victims
##     &lt;int&gt;
## 1      37</code></pre>
<pre class="r"><code>Merged_Data %&gt;% select(state_abbrev, raceethnicity, armed)</code></pre>
<pre><code>## # A tibble: 400 x 3
##    state_abbrev raceethnicity   armed  
##    &lt;chr&gt;        &lt;chr&gt;           &lt;chr&gt;  
##  1 AL           Black           No     
##  2 AL           Black           Knife  
##  3 AL           Black           No     
##  4 AL           White           Firearm
##  5 AL           White           Firearm
##  6 AL           White           Other  
##  7 AL           Black           Firearm
##  8 AL           White           No     
##  9 AK           Hispanic/Latino Knife  
## 10 AK           Native American Knife  
## # … with 390 more rows</code></pre>
<pre class="r"><code>Merged_Data %&gt;% arrange(-avg_hatecrimes_per_100k_fbi) %&gt;% select(state_abbrev, 
    avg_hatecrimes_per_100k_fbi) %&gt;% distinct(state_abbrev, .keep_all = TRUE)</code></pre>
<pre><code>## # A tibble: 43 x 2
##    state_abbrev avg_hatecrimes_per_100k_fbi
##    &lt;chr&gt;                              &lt;dbl&gt;
##  1 DC                                 11.0 
##  2 MA                                  4.80
##  3 NJ                                  4.41
##  4 KY                                  4.21
##  5 WA                                  3.82
##  6 CT                                  3.77
##  7 MN                                  3.61
##  8 AZ                                  3.41
##  9 OR                                  3.39
## 10 OH                                  3.24
## # … with 33 more rows</code></pre>
<pre class="r"><code>Merged_Data %&gt;% group_by(state_abbrev, raceethnicity) %&gt;% summarize(count = n())</code></pre>
<pre><code>## # A tibble: 95 x 3
## # Groups:   state_abbrev [43]
##    state_abbrev raceethnicity   count
##    &lt;chr&gt;        &lt;chr&gt;           &lt;int&gt;
##  1 AK           Hispanic/Latino     1
##  2 AK           Native American     1
##  3 AL           Black               4
##  4 AL           White               4
##  5 AR           Black               1
##  6 AR           White               2
##  7 AZ           Black               1
##  8 AZ           Hispanic/Latino     6
##  9 AZ           Native American     2
## 10 AZ           White              13
## # … with 85 more rows</code></pre>
<pre class="r"><code>Merged_Data %&gt;% mutate(share_non_poverty_white = 1 - share_white_poverty)</code></pre>
<pre><code>## # A tibble: 400 x 47
##    state state_abbrev median_house_inc share_unemp_seas share_pop_metro
##    &lt;chr&gt; &lt;chr&gt;                   &lt;int&gt;            &lt;dbl&gt;           &lt;dbl&gt;
##  1 Alab… AL                      42278            0.06             0.64
##  2 Alab… AL                      42278            0.06             0.64
##  3 Alab… AL                      42278            0.06             0.64
##  4 Alab… AL                      42278            0.06             0.64
##  5 Alab… AL                      42278            0.06             0.64
##  6 Alab… AL                      42278            0.06             0.64
##  7 Alab… AL                      42278            0.06             0.64
##  8 Alab… AL                      42278            0.06             0.64
##  9 Alas… AK                      67629            0.064            0.63
## 10 Alas… AK                      67629            0.064            0.63
## # … with 390 more rows, and 42 more variables: share_pop_hs &lt;dbl&gt;,
## #   share_non_citizen &lt;dbl&gt;, share_white_poverty &lt;dbl&gt;, gini_index &lt;dbl&gt;,
## #   share_non_white &lt;dbl&gt;, share_vote_trump &lt;dbl&gt;,
## #   hate_crimes_per_100k_splc &lt;dbl&gt;, avg_hatecrimes_per_100k_fbi &lt;dbl&gt;,
## #   name &lt;chr&gt;, age &lt;int&gt;, gender &lt;chr&gt;, raceethnicity &lt;chr&gt;, month &lt;chr&gt;,
## #   day &lt;int&gt;, year &lt;int&gt;, streetaddress &lt;chr&gt;, city &lt;chr&gt;, latitude &lt;dbl&gt;,
## #   longitude &lt;dbl&gt;, state_fp &lt;int&gt;, county_fp &lt;int&gt;, tract_ce &lt;int&gt;,
## #   geo_id &lt;dbl&gt;, county_id &lt;int&gt;, namelsad &lt;chr&gt;, lawenforcementagency &lt;chr&gt;,
## #   cause &lt;chr&gt;, armed &lt;chr&gt;, pop &lt;int&gt;, share_white &lt;dbl&gt;, share_black &lt;dbl&gt;,
## #   share_hispanic &lt;dbl&gt;, p_income &lt;int&gt;, h_income &lt;int&gt;, county_income &lt;int&gt;,
## #   comp_income &lt;dbl&gt;, county_bucket &lt;int&gt;, nat_bucket &lt;int&gt;, pov &lt;dbl&gt;,
## #   urate &lt;dbl&gt;, college &lt;dbl&gt;, share_non_poverty_white &lt;dbl&gt;</code></pre>
<pre class="r"><code>Merged_Data %&gt;% summarize(min(avg_hatecrimes_per_100k_fbi), max(avg_hatecrimes_per_100k_fbi))</code></pre>
<pre><code>## # A tibble: 1 x 2
##   `min(avg_hatecrimes_per_100k_fbi)` `max(avg_hatecrimes_per_100k_fbi)`
##                                &lt;dbl&gt;                              &lt;dbl&gt;
## 1                              0.412                               11.0</code></pre>
<pre class="r"><code># Summary Statistics (Mean, Median, Minimum, Maximum, and
# Standard Deviation) 1-5
Merged_Data %&gt;% summarize(mean_hs_diploma = mean(share_pop_hs), 
    median_hs_diploma = median(share_pop_hs), minimum_hs_diploma = min(share_pop_hs), 
    maximum_hs_diploma = max(share_pop_hs), Std_Dev_hs_diploma = sd(share_pop_hs))</code></pre>
<pre><code>## # A tibble: 1 x 5
##   mean_hs_diploma median_hs_diplo… minimum_hs_dipl… maximum_hs_dipl…
##             &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;
## 1           0.847            0.845            0.799            0.915
## # … with 1 more variable: Std_Dev_hs_diploma &lt;dbl&gt;</code></pre>
<pre class="r"><code>Merged_Data %&gt;% summarize(mean_median_house_income = mean(median_house_inc), 
    median_median_house_income = median(median_house_inc), minimum_median_house_income = min(median_house_inc), 
    maximum_median_house_income = max(median_house_inc), Std_Dev_median_house_income = sd(median_house_inc))</code></pre>
<pre><code>## # A tibble: 1 x 5
##   mean_median_hou… median_median_h… minimum_median_… maximum_median_…
##              &lt;dbl&gt;            &lt;dbl&gt;            &lt;int&gt;            &lt;int&gt;
## 1           54415.            53875            39552            76165
## # … with 1 more variable: Std_Dev_median_house_income &lt;dbl&gt;</code></pre>
<pre class="r"><code>Merged_Data %&gt;% summarize(mean_unemployed = mean(share_unemp_seas), 
    median_unemployed = median(share_unemp_seas), minimum_unemployed = min(share_unemp_seas), 
    maximum_unemployed = max(share_unemp_seas), Std_Dev_unemployed = sd(share_unemp_seas))</code></pre>
<pre><code>## # A tibble: 1 x 5
##   mean_unemployed median_unemploy… minimum_unemplo… maximum_unemplo…
##             &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;
## 1          0.0522            0.052            0.029            0.073
## # … with 1 more variable: Std_Dev_unemployed &lt;dbl&gt;</code></pre>
<pre class="r"><code>Merged_Data %&gt;% summarize(mean_non_citizen = mean(share_non_citizen), 
    median_non_citizen = median(share_non_citizen), minimum_non_citizen = min(share_non_citizen), 
    maximum_non_citizen = max(share_non_citizen), Std_Dev_non_citizen = sd(share_non_citizen))</code></pre>
<pre><code>## # A tibble: 1 x 5
##   mean_non_citizen median_non_citi… minimum_non_cit… maximum_non_cit…
##              &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;
## 1           0.0760             0.08             0.01             0.13
## # … with 1 more variable: Std_Dev_non_citizen &lt;dbl&gt;</code></pre>
<pre class="r"><code>Merged_Data %&gt;% summarize(mean_non_white = mean(share_non_white), 
    median_non_white = median(share_non_white), minimum_non_white = min(share_non_white), 
    maximum_non_white = max(share_non_white), Std_Dev_non_white = sd(share_non_white))</code></pre>
<pre><code>## # A tibble: 1 x 5
##   mean_non_white median_non_white minimum_non_whi… maximum_non_whi…
##            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;
## 1          0.411             0.42             0.07             0.63
## # … with 1 more variable: Std_Dev_non_white &lt;dbl&gt;</code></pre>
<pre class="r"><code># 6-10
Merged_Data %&gt;% summarize(mean_voted_trump = mean(share_vote_trump), 
    median_voted_trump = median(share_vote_trump), minimum_voted_trump = min(share_vote_trump), 
    maximum_voted_trump = max(share_vote_trump), Std_Dev_voted_trump = sd(share_vote_trump))</code></pre>
<pre><code>## # A tibble: 1 x 5
##   mean_voted_trump median_voted_tr… minimum_voted_t… maximum_voted_t…
##              &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;
## 1            0.472             0.49             0.04             0.69
## # … with 1 more variable: Std_Dev_voted_trump &lt;dbl&gt;</code></pre>
<pre class="r"><code>Merged_Data %&gt;% summarize(mean_avg_hatecrimes_btwn_2010_2015 = mean(avg_hatecrimes_per_100k_fbi), 
    median_avg_hatecrimes_btwn_2010_2015 = median(avg_hatecrimes_per_100k_fbi), 
    minimum_avg_hatecrimes_btwn_2010_2015 = min(avg_hatecrimes_per_100k_fbi), 
    maximum_avg_hatecrimes_btwn_2010_2015 = max(avg_hatecrimes_per_100k_fbi), 
    Std_Dev_avg_hatecrimes_btwn_2010_2015 = sd(avg_hatecrimes_per_100k_fbi))</code></pre>
<pre><code>## # A tibble: 1 x 5
##   mean_avg_hatecr… median_avg_hate… minimum_avg_hat… maximum_avg_hat…
##              &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;
## 1             2.09             2.04            0.412             11.0
## # … with 1 more variable: Std_Dev_avg_hatecrimes_btwn_2010_2015 &lt;dbl&gt;</code></pre>
<pre class="r"><code>Merged_Data %&gt;% summarize(mean_gini_index = mean(gini_index), 
    median_gini_index = median(gini_index), minimum_gini_index = min(gini_index), 
    maximum_gini_index = max(gini_index), Std_Dev_gini_index = sd(gini_index))</code></pre>
<pre><code>## # A tibble: 1 x 5
##   mean_gini_index median_gini_ind… minimum_gini_in… maximum_gini_in…
##             &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;
## 1           0.461            0.464            0.419            0.532
## # … with 1 more variable: Std_Dev_gini_index &lt;dbl&gt;</code></pre>
<pre class="r"><code>Merged_Data %&gt;% group_by(raceethnicity) %&gt;% summarize(mean_avg_hatecrimes_btwn_2010_2015 = mean(avg_hatecrimes_per_100k_fbi), 
    median_avg_hatecrimes_btwn_2010_2015 = median(avg_hatecrimes_per_100k_fbi), 
    minimum_avg_hatecrimes_btwn_2010_2015 = min(avg_hatecrimes_per_100k_fbi), 
    maximum_avg_hatecrimes_btwn_2010_2015 = max(avg_hatecrimes_per_100k_fbi), 
    Std_Dev_avg_hatecrimes_btwn_2010_2015 = sd(avg_hatecrimes_per_100k_fbi))</code></pre>
<pre><code>## # A tibble: 5 x 6
##   raceethnicity mean_avg_hatecr… median_avg_hate… minimum_avg_hat…
##   &lt;chr&gt;                    &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;
## 1 Asian/Pacifi…             2.44             2.40            0.753
## 2 Black                     1.94             1.72            0.412
## 3 Hispanic/Lat…             2.11             2.40            0.412
## 4 Native Ameri…             3.08             3.41            1.66 
## 5 White                     2.15             2.11            0.412
## # … with 2 more variables: maximum_avg_hatecrimes_btwn_2010_2015 &lt;dbl&gt;,
## #   Std_Dev_avg_hatecrimes_btwn_2010_2015 &lt;dbl&gt;</code></pre>
<pre class="r"><code>Merged_Data %&gt;% group_by(raceethnicity, armed) %&gt;% summarize(mean_household_income = mean(h_income), 
    median_household_income = median(h_income), minimum_household_income = min(h_income), 
    maximum_household_income = max(h_income), Std_Dev_household_income = sd(h_income))</code></pre>
<pre><code>## # A tibble: 26 x 7
## # Groups:   raceethnicity [5]
##    raceethnicity armed mean_household_… median_househol… minimum_househo…
##    &lt;chr&gt;         &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;int&gt;
##  1 Asian/Pacifi… Fire…           63936.           41385             38958
##  2 Asian/Pacifi… Knife           36356            36356             15212
##  3 Asian/Pacifi… No              46206.           46206.            41786
##  4 Asian/Pacifi… Vehi…           49973            49973             49973
##  5 Black         Fire…           40209.           35616.            11378
##  6 Black         Knife           43986.           50268             19988
##  7 Black         No              48151.           35993             17545
##  8 Black         Non-…           63444            63444             63444
##  9 Black         Other           40863            36070             20556
## 10 Black         Vehi…           36387.           37109             10290
## # … with 16 more rows, and 2 more variables: maximum_household_income &lt;int&gt;,
## #   Std_Dev_household_income &lt;dbl&gt;</code></pre>
<p>Using the six core dplyr functions, I manipulated the data in order to discover various results. Filtering only for Texas, I found that 37 people were killed in 2015. Using arrange, I found that D.C. has the highest average amount of hate crimes per 100k people during the stretch of 2010-2015. In fact, 7 of the top 10 states with the highest average hate crimes per 100k people were states on the Eastern side of the United states, with the 3 Western states being Washington, Arizona, and Oregon respectively. Texas, surprisingly, ranks near the bottom at 0.7527683 hate crimes per 100k people. Using group_by, I organized the data to display each state and the amount of people killed due to police brutality per race/ethnicity. In 2015, California found itself at the top in the number of people killed for each race, with Asian/Pacific Islander people at 3, Black people at 14, Hispanic/Latino people at 25, and White people at 27. Using summarize, through the stretch of 2010-2015, the lowest average hate crimes committed per 100k people for a state was 0.4120118 and the highest was 10.95348.
Using the summary statistics of mean, median, min, max, and standard deviation, I found interesting results. The average proportion of non-white citizens in a state was 0.411475 of the population, the median was 0.42, the minimum of any state was 0.07, the maximum of any state was 0.63, and the standard deviation was 0.1486803. The average proportion of voting citizens who voted for Trump in the 2016 election in all states was 0.47215, the median was 0.49, the minimum of any state was 0.04, the maximum of any state was 0.69, and the standard deviation was 0.1008877. These two results represents the massive divide in this country. A state can have a non-white population of anywhere from 7% of the population to 63% of the population. This huge range shows the difference in an individual who grows up and lives with a large amount of diversity and an individual who doesn’t, and could potentially account for why hate crimes and police brutality are in an uptick as of recently. Grouping by the race/ethnicity and by whether the victim was armed or not, it can be seen that the average household income for unarmed Hispanic/Latino and Native American Victims were the lowest compared to if they were armed.</p>
</div>
<div id="visualizing" class="section level2">
<h2>4. Visualizing</h2>
<pre class="r"><code>mydata &lt;- Merged_Data %&gt;% select(-year, -urate, -tract_ce, -state_fp, 
    -pop, -pov, -p_income, -nat_bucket, -longitude, -latitude, 
    -geo_id, -day, -county_bucket, -county_id, -county_fp, -college, 
    -age) %&gt;% select_if(is_numeric)
cormat &lt;- mydata %&gt;% cor(use = &quot;pair&quot;)
cormat &lt;- cormat %&gt;% as.data.frame %&gt;% rownames_to_column(&quot;Var1&quot;) %&gt;% 
    pivot_longer(., cols = 2:ncol(.), names_to = &quot;Var2&quot;, values_to = &quot;Correlation&quot;)
cormat %&gt;% ggplot(aes(Var1, Var2, fill = Correlation)) + geom_tile(color = &quot;white&quot;) + 
    theme(axis.text.y = element_text(size = 9)) + theme(axis.text.x = element_text(angle = 90, 
    hjust = 1, size = 9)) + coord_fixed() + labs(title = &quot;Data from Hate Crimes and Police Brutality&quot;)</code></pre>
<p><img src="../../project/project1_files/figure-html/unnamed-chunk-4-1.png" width="672" style="display: block; margin: auto;" />
This correlation heatmap shows that there is a relationship between the proportion of White people in a population and the average income of the household, in which the higher the proportion the higher the average income of the household. Inversely, the higher the proportion of Black people in a population the lower the average income of the household. There is also a high correlation between the proportion of impoverished White population and the proportion of the voting population who voted for Trump in the 2016 election.</p>
<pre class="r"><code>Merged_Data %&gt;% ggplot(aes(x = state_abbrev, y = h_income, color = raceethnicity)) + 
    geom_point(stat = &quot;summary&quot;, size = 2) + labs(x = &quot;State&quot;, 
    y = &quot;Household Income&quot;, color = &quot;Race/Ethnicity&quot;, title = &quot;Household Income Of Victim With Respect To Race And State&quot;) + 
    theme_bw() + theme(axis.text.x = element_text(angle = 90, 
    size = 9)) + scale_y_continuous(breaks = seq(0, 160000, by = 20000))</code></pre>
<p><img src="../../project/project1_files/figure-html/unnamed-chunk-5-1.png" width="672" style="display: block; margin: auto;" />
This plot presents the household income of each individual that was slain by police with respect to their race/ethnicity and the state it occurred in. This plot shows that Black individuals that are killed by police typically have a lower pay than the White individuals who were also killed. This can be interpreted in that violent police action will occur more often in impoverished Black neighborhoods than impoverished White neighborhoods across the United States. There is much more fatal police action to lower income individuals than the comparatively higher income individuals. This can be interpreted that fatal police action is more often seen in poorer areas of the United States.</p>
<pre class="r"><code>Merged_Data %&gt;% ggplot(aes(x = raceethnicity, fill = armed)) + 
    geom_bar(position = &quot;dodge&quot;) + labs(x = &quot;Race/Ethnicity&quot;, 
    y = &quot;# of People Killed&quot;, color = &quot;Armed&quot;, title = &quot;Victim&#39;s Race and If They Were Armed&quot;) + 
    scale_y_continuous(breaks = seq(0, 140, by = 20))</code></pre>
<p><img src="../../project/project1_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" />
This plot shows the victim’s race and whether they were armed or not. A clear pattern to be seen is that most people of each race/ethnicity that were killed by police had a firearm on them. While it can be said that White individuals are killed the most by police, they are also the race/ethnicity that most frequently has a firearm with them at the time of their murder. Despite having less overall deaths compared to White individuals, Black individuals are just slightly below in the amount of unarmed deaths. Asian/Pacific Islander and Native American individuals see generally the same trend in which victims with firearms account for the highest number of deaths respective to their race/ethnicity, however they also see the lowest total amount of deaths. It should also be noted that Black individuals encounter the highest relative deaths with their armament being a vehicle compared to the other races/ethnicities.</p>
</div>
<div id="dimension-reduction" class="section level2">
<h2>5. Dimension Reduction</h2>
<pre class="r"><code>clust_dat &lt;- Merged_Data %&gt;% dplyr::select(share_vote_trump, 
    share_white_poverty, avg_hatecrimes_per_100k_fbi)
set.seed(348)
sil_width &lt;- vector()  #empty vector to hold mean sil width
for (i in 2:10) {
    kms &lt;- kmeans(clust_dat, centers = i)  #compute k-means solution
    sil &lt;- silhouette(kms$cluster, dist(clust_dat))  #get sil widths
    sil_width[i] &lt;- mean(sil[, 3])  #take averages (higher is better)
}
ggplot() + geom_line(aes(x = 1:10, y = sil_width)) + scale_x_continuous(name = &quot;k&quot;, 
    breaks = 1:10) + labs(x = &quot;Number of Clusters&quot;, y = &quot;Silhouette Width&quot;, 
    title = &quot;Determining How Many Clusters&quot;)</code></pre>
<p><img src="../../project/project1_files/figure-html/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>fit &lt;- kmeans(clust_dat, 9)
clusplot(clust_dat, fit$cluster, color = TRUE, shade = TRUE, 
    labels = 1, lines = 0)</code></pre>
<p><img src="../../project/project1_files/figure-html/unnamed-chunk-7-2.png" width="672" style="display: block; margin: auto;" />
As seen in the first plot, the number of clusters to use is 9 because of the highest silhouette width. I found that there is a relation between the proportion of the voting population who voted for Trump in the 2016 election, the proportion of the White population who is in poverty, and the average number of hate crimes committed between 2010 and 2015 per 100k people. There is an 84.57% explanation of the point variability, which is very strong.</p>
</div>

            
        <hr>         <div class="related-posts">
                <h5>Related Posts</h5>
                
              </div> 
            </div>
          </div>

   <hr>  <div class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">

    (function() {
      
      
      if (window.location.hostname == "localhost")
        return;

      var disqus_shortname = '';
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div> 
        </div>
      </div>
    </div>

    
    <footer>
  <div id="footer">
    <div class="container">
      <p class="text-muted">&copy; All rights reserved. Powered by <a href="https://gohugo.io/">Hugo</a> and
      <a href="http://www.github.com/nurlansu/hugo-sustain/">sustain</a> with ♥</p>
    </div>
  </div>
</footer>
<div class="footer"></div>


<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="../../js/docs.min.js"></script>
<script src="../../js/main.js"></script>

<script src="../../js/ie10-viewport-bug-workaround.js"></script>


    
  </body>
</html>
